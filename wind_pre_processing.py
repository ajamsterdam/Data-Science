# -*- coding: utf-8 -*-
"""Wind Pre-processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f3tyoLPEg_XfJWbtGsGHJjYlGnpPW0Rx
"""

!pip install netCDF4 xarray openpy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import os, sys
import netCDF4
import pyproj
from pyproj import Proj
import datetime
import warnings
warnings.filterwarnings('ignore')



from google.colab import drive
drive.mount('/content/drive')

from netCDF4 import Dataset

import os

# Step 1: List files and directories in the parent directory
parent_directory_path = '/content/drive/MyDrive/'
try:
    parent_contents = os.listdir(parent_directory_path)
    print("Parent Directory Contents:", parent_contents)
except FileNotFoundError:
    print(f"Parent directory not found: {parent_directory_path}")

# Step 2: Verify the specific subdirectory and list its contents
subdirectory_name = 'Sailing Weather Model'
directory_path = os.path.join(parent_directory_path, subdirectory_name)
if subdirectory_name in parent_contents:
    try:
        directory_contents = os.listdir(directory_path)
        print("Directory Contents:", directory_contents)
    except FileNotFoundError:
        print(f"Directory not found: {directory_path}")
else:
    print(f"Subdirectory '{subdirectory_name}' not found in parent directory.")

# Step 3: Verify the specific file path
file_path = '/content/drive/MyDrive/Sailing Weather Model/Weather 2023.nc'
if os.path.isfile(file_path):
    print(f"File found: {file_path}")
else:
    print(f"File not found: {file_path}")

file_path = '/content/drive/MyDrive/Sailing Weather Model/Weather 2023.nc'

import os
directory_path = '/content/drive/MyDrive/Sailing Weather Model'
print(os.listdir(directory_path))

import xarray as xr

# Specify the file path
file_path = '/content/drive/MyDrive/Sailing Weather Model/Weather 2023.nc'
file_path = '/content/drive/MyDrive/Sailing Weather Model/Weather 2022.nc'
file_path = '/content/drive/MyDrive/Sailing Weather Model/Weather 2021.nc'
file_path = '/content/drive/MyDrive/Sailing Weather Model/Weather 2020.nc'

file_path = '/content/drive/MyDrive/Sailing Weather Model/Weather 2023.nc'
file_path = '/content/drive/MyDrive/Sailing Weather Model/Weather 2022.nc'
file_path = '/content/drive/MyDrive/Sailing Weather Model/Weather 2021.nc'
file_path = '/content/drive/MyDrive/Sailing Weather Model/Weather 2020.nc'

if os.path.exists(file_path):
    print("File exists at path:", file_path)
else:
    print("File does not exist at path:", file_path)

import os

directory_path = '/content/drive/MyDrive/Sailing Weather Model'
files = os.listdir(directory_path)
print("Files in directory:", files)

file_path = '/content/drive/MyDrive/Sailing Weather Model/Weather 2023.nc'
print("File path being checked:", file_path)

if os.path.exists(file_path):
    print("File exists at path:", file_path)
else:
    print("File does not exist at path:", file_path)

import netCDF4

try:
    dataset = netCDF4.Dataset(file_path, 'r')
    print("Dataset loaded successfully.")
except FileNotFoundError:
    print("FileNotFoundError: The file was not found at path:", file_path)
except PermissionError:
    print("PermissionError: Permission denied to access the file at path:", file_path)
except Exception as e:
    print("An unexpected error occurred:", e)

import os

# Define the base directory path with the correct subdirectory name
base_directory_path = '/content/drive/MyDrive/Sailing Weather Model'

# List the files to check
file_names = [
    'Weather 2020.nc',
    'Weather 2021.nc',
    'Weather 2022.nc',
    'Weather 2023.nc'
]

# Check and load each file
for file_name in file_names:
    file_path = os.path.join(base_directory_path, file_name)
    print("File path being checked:", file_path)
    if os.path.exists(file_path):
        print("File exists at path:", file_path)
        # Load the file (assuming loading means reading into a netCDF4 dataset)
        dataset = netCDF4.Dataset(file_path, 'r')
        print(f"Successfully loaded {file_name}")
        # If you need to perform specific actions with the loaded dataset, do so here
    else:
        print("File does not exist at path:", file_path)



# Print dataset summary
print(dataset)

# List all variables
print(dataset.variables.keys())

# Get dimensions of a specific variable
temperature_2m = dataset.variables['t2m']
print(temperature_2m.shape)

print(dataset)

import xarray as xr

# Convert the netCDF4 dataset to xarray dataset
xr_dataset = xr.open_dataset(xr.backends.NetCDF4DataStore(dataset))

# Print dataset to understand its structure
print(xr_dataset)

!pip install xarray dask netCDF4

import xarray as xr
import dask
import os

# Define the base directory path with the correct subdirectory name
base_directory_path = '/content/drive/MyDrive/Sailing Weather Model'

# List the files to check
file_names = [
    'Weather 2020.nc',
    'Weather 2021.nc',
    'Weather 2022.nc',
    'Weather 2023.nc'
]

# Function to load a dataset with dask
def load_dataset(file_path):
    return xr.open_dataset(file_path, chunks={'time': 1000})

# Load the datasets using dask
datasets = []
for file_name in file_names:
    file_path = os.path.join(base_directory_path, file_name)
    if os.path.exists(file_path):
        print(f"Loading file: {file_path}")
        ds = load_dataset(file_path)
        datasets.append(ds)
    else:
        print(f"File not found: {file_path}")

# Concatenate datasets along the time dimension using dask
if datasets:
    combined_ds = xr.concat(datasets, dim='time')
    print("Datasets concatenated successfully")

    # Optionally, persist the combined dataset to optimize performance
    combined_ds = combined_ds.persist()

    # Save the combined dataset to a new NetCDF file
    output_path = os.path.join(base_directory_path, 'Combined_Weather_2020_2023.nc')
    combined_ds.to_netcdf(output_path)
    print(f"Combined dataset saved to: {output_path}")
else:
    print("No datasets were loaded, please check file paths and names.")

import xarray as xr

# Load the combined dataset
combined_file_path = '/content/drive/MyDrive/Sailing Weather Model/Combined_Weather_2020_2023.nc'
combined_ds = xr.open_dataset(combined_file_path)

# Inspect the dataset
print(combined_ds)

# Check the shape of the dataset
# Print the shape of each variable
for var in combined_ds.data_vars:
    print(f"Shape of {var}: {combined_ds[var].shape}")

# If you want to check the overall dimensions
print("Overall dimensions:", combined_ds.dims)

# Example of accessing specific data
# Select a variable (e.g., t2m) and check its values and shape
t2m_data = combined_ds['t2m']
print("t2m data shape:", t2m_data.shape)
print("t2m data values:", t2m_data.values)

# Perform basic statistics
t2m_mean = t2m_data.mean(dim='time')
print("Mean t2m over time:", t2m_mean.values)

# Visualize data (requires matplotlib)
import matplotlib.pyplot as plt

t2m_mean.plot()
plt.title('Mean 2m Temperature')
plt.show()

import xarray as xr

# Load the combined dataset
combined_file_path = '/content/drive/MyDrive/Sailing Weather Model/Combined_Weather_2020_2023.nc'
combined_ds = xr.open_dataset(combined_file_path)

# Print the dataset to inspect the variables
print(combined_ds)

# Function to compute descriptive statistics
def compute_descriptive_stats(ds):
    stats = {}
    for var in ds.data_vars:
        data = ds[var]
        stats[var] = {
            'count': data.notnull().sum().item(),
            'mean': data.mean().item(),
            'std': data.std().item(),
            'min': data.min().item(),
            '25%': data.quantile(0.25).item(),
            '50%': data.quantile(0.50).item(),
            '75%': data.quantile(0.75).item(),
            'max': data.max().item()
        }
    return stats

# Compute statistics
stats = compute_descriptive_stats(combined_ds)

# Convert the statistics to a DataFrame for a better view
import pandas as pd
stats_df = pd.DataFrame(stats).T
print(stats_df)

# List the columns (variables) in the already loaded dataset combined_ds
columns = list(combined_ds.data_vars)
print("Columns (variables) in the dataset:")
print(columns)

# Step 1: Identify Missing Values
# Check for missing values
print("Missing values in the dataset:")
print(combined_ds.isnull().sum())

# Handle missing values: Fill missing values with the mean or use interpolation
combined_ds = combined_ds.fillna(combined_ds.mean())

# Step 2: Handle Outliers
# Define a function to identify and handle outliers
def remove_outliers(ds, variable, lower_quantile=0.01, upper_quantile=0.99):
    q_low = ds[variable].quantile(lower_quantile)
    q_high = ds[variable].quantile(upper_quantile)
    ds[variable] = ds[variable].where((ds[variable] >= q_low) & (ds[variable] <= q_high), other=np.nan)
    ds[variable] = ds[variable].fillna(ds[variable].mean())
    return ds

# Apply the function to each relevant variable
variables_to_clean = ['t2m', 'u100', 'v100', 'u10', 'v10', 'fg10', 'i10fg', 'msl', 'sp', 'cbh', 'tcc']
for var in variables_to_clean:
    combined_ds = remove_outliers(combined_ds, var)

# Step 3: Ensure Data Consistency
# Ensure that data types are consistent
for var in combined_ds.data_vars:
    combined_ds[var] = combined_ds[var].astype(np.float32)

# Step 4: Calculate New Variables
# Calculate the Coriolis parameter (f) based on latitude
omega = 7.2921e-5  # Earth's angular velocity in rad/s
combined_ds['coriolis'] = 2 * omega * np.sin(np.deg2rad(combined_ds['latitude']))
print("Coriolis parameter calculated.")

# Calculate wind speed from U and V components
combined_ds['wind_speed_100m'] = np.sqrt(combined_ds['u100']**2 + combined_ds['v100']**2)
combined_ds['wind_speed_10m'] = np.sqrt(combined_ds['u10']**2 + combined_ds['v10']**2)
print("Wind Speed calculated.")

# Calculate wind direction from U and V components
combined_ds['wind_direction_100m'] = np.arctan2(combined_ds['u100'], combined_ds['v100'])
combined_ds['wind_direction_10m'] = np.arctan2(combined_ds['u10'], combined_ds['v10'])
print("Wind Direction calculated.")

# Inspect the new variables
print(combined_ds[['coriolis', 'wind_speed_100m', 'wind_speed_10m', 'wind_direction_100m', 'wind_direction_10m']])

# Example plot
combined_ds['wind_speed_10m'].isel(time=0).plot()
plt.title('Wind Speed at 10m for First Time Step')
plt.show()

# If needed, compute descriptive statistics and convert to a DataFrame
def compute_descriptive_stats(ds):
    stats = {}
    for var in ds.data_vars:
        data = ds[var]
        stats[var] = {
            'count': data.notnull().sum().item(),
            'mean': data.mean().item(),
            'std': data.std().item(),
            'min': data.min().item(),
            '25%': data.quantile(0.25).item(),
            '50%': data.quantile(0.50).item(),
            '75%': data.quantile(0.75).item(),
            'max': data.max().item()
        }
    return stats

# Compute statistics
stats = compute_descriptive_stats(combined_ds)

# Convert the statistics to a DataFrame for a better view
stats_df = pd.DataFrame(stats).T
print(stats_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Convert xarray dataset to pandas DataFrame
df = combined_ds.to_dataframe().reset_index()

# Select relevant variables
relevant_vars = ['u100', 'v100', 'u10', 'v10', 'fg10', 't2m', 'cbh', 'i10fg', 'mdww',
                 'msl', 'mslhf', 'msshf', 'mwd', 'dwi', 'wind', 'shww', 'sp', 'tcc',
                 'coriolis', 'wind_speed_100m', 'wind_speed_10m', 'wind_direction_100m',
                 'wind_direction_10m']

# Boxplots
plt.figure(figsize=(20, 15))
for i, var in enumerate(relevant_vars, 1):
    plt.subplot(6, 4, i)
    sns.boxplot(data=df, x=var)
    plt.title(f'Boxplot of {var}')
plt.tight_layout()
plt.show()

# Histograms
plt.figure(figsize=(20, 15))
for i, var in enumerate(relevant_vars, 1):
    plt.subplot(6, 4, i)
    sns.histplot(df[var], bins=30, kde=True)
    plt.title(f'Histogram of {var}')
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 'df' = DataFrame after loading and resetting index
# Dropping non-numeric columns for PCA
numeric_df = df.select_dtypes(include=[float, int])

# Normalizing the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_df)

# Applying PCA
pca = PCA(n_components=0.95)  # Retain 95% of the variance
principal_components = pca.fit_transform(scaled_data)

# Creating a DataFrame with the principal components
pca_df = pd.DataFrame(data=principal_components)

# Saving the PCA-transformed data to a CSV file for further analysis
pca_df.to_csv('pca_transformed_data.csv', index=False)

# Checking the shape of the resulting DataFrame
print("Shape of the PCA-transformed DataFrame:", pca_df.shape)
print("First few rows of the PCA-transformed DataFrame:\n", pca_df.head())

# Compute the correlation matrix
corr_matrix = df[relevant_vars].corr()

# Plot the heatmap
plt.figure(figsize=(15, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix Heatmap')
plt.show()

import os
import netCDF4
import xarray as xr
import warnings
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

warnings.filterwarnings('ignore')

# Load the combined dataset for analysis
combined_file_path = '/content/drive/MyDrive/Sailing Weather Model/Combined_Weather_2020_2023.nc'
combined_ds = xr.open_dataset(combined_file_path)

# Select the focus period (December to April) and variables
combined_ds = combined_ds.sel(time=combined_ds.time.dt.month.isin([12, 1, 2, 3, 4]))
variables = ['u100', 'v100', 'u10', 'v10', 'fg10', 't2m', 'cbh', 'i10fg', 'mdww', 'msl', 'mslhf', 'msshf', 'mwd', 'dwi', 'wind', 'shww', 'sp', 'tcc']
combined_ds = combined_ds[variables]

# Print the dimensions of the selected dataset
print("Dimensions of the selected dataset:", combined_ds.dims)

# Check for required variables in the xarray dataset
required_vars = ['coriolis', 'wind_speed_100m', 'wind_speed_10m', 'wind_direction_100m', 'wind_direction_10m']
missing_vars = [var for var in required_vars if var not in combined_ds.variables]

# Calculate missing variables if necessary
if 'coriolis' not in combined_ds.variables:
    omega = 7.2921e-5  # Earth's angular velocity in rad/s
    combined_ds['coriolis'] = 2 * omega * np.sin(np.deg2rad(combined_ds['latitude']))

if 'wind_speed_100m' not in combined_ds.variables:
    combined_ds['wind_speed_100m'] = np.sqrt(combined_ds['u100']**2 + combined_ds['v100']**2)

if 'wind_speed_10m' not in combined_ds.variables:
    combined_ds['wind_speed_10m'] = np.sqrt(combined_ds['u10']**2 + combined_ds['v10']**2)

if 'wind_direction_100m' not in combined_ds.variables:
    combined_ds['wind_direction_100m'] = np.arctan2(combined_ds['u100'], combined_ds['v100'])

if 'wind_direction_10m' not in combined_ds.variables:
    combined_ds['wind_direction_10m'] = np.arctan2(combined_ds['u10'], combined_ds['v10'])

# Convert xarray dataset to pandas DataFrame
df = combined_ds.to_dataframe().reset_index()

# Print the columns to verify variable names
print("Columns in the DataFrame:", df.columns)

# Define relevant variables
relevant_vars = ['u100', 'v100', 'u10', 'v10', 'fg10', 't2m', 'cbh', 'i10fg', 'mdww', 'msl',
                 'mslhf', 'msshf', 'mwd', 'dwi', 'wind', 'shww', 'sp', 'tcc', 'coriolis',
                 'wind_speed_100m', 'wind_speed_10m', 'wind_direction_100m', 'wind_direction_10m']

# Check if the relevant variables exist in the DataFrame
for var in relevant_vars:
    if var not in df.columns:
        print(f"Warning: {var} not found in DataFrame")

# Filter only the columns that exist in the DataFrame
relevant_vars = [var for var in relevant_vars if var in df.columns]

# Handle outliers
def remove_outliers_iqr(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

outlier_columns = ['cbh', 'mdww', 'mslhf', 'msshf', 'sp', 'wind_direction_100m', 'wind_direction_10m']
outlier_columns = [col for col in outlier_columns if col in df.columns]
df_cleaned = remove_outliers_iqr(df, outlier_columns)

# Apply transformations
if 'cbh' in df_cleaned.columns:
    df_cleaned['cbh_log'] = np.log1p(df_cleaned['cbh'])
if 'msshf' in df_cleaned.columns:
    df_cleaned['msshf_sqrt'] = np.sqrt(df_cleaned['msshf'])
if 'tcc' in df_cleaned.columns:
    df_cleaned['tcc_transformed'] = df_cleaned['tcc'].apply(lambda x: np.log1p(x) if x > 0 else 0)

# Normalize data
scaler = RobustScaler()
scaled_df = scaler.fit_transform(df_cleaned[relevant_vars])
scaled_df = pd.DataFrame(scaled_df, columns=relevant_vars)

# Final DataFrame for model training
print(scaled_df.head())

# Proceed with LSTM model training as previously outlined

scaled_data.shape

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Ensure GPU is available
if not tf.config.list_physical_devices('GPU'):
    raise SystemError('GPU device not found')
else:
    print('Found GPU at:', tf.test.gpu_device_name())

# Assuming 'scaled_df' and 'df_cleaned' are already prepared as before
# Define relevant variables again if needed
relevant_vars = ['u100', 'v100', 'u10', 'v10', 'fg10', 't2m', 'cbh', 'i10fg', 'mdww', 'msl',
                 'mslhf', 'msshf', 'mwd', 'dwi', 'wind', 'shww', 'sp', 'tcc', 'coriolis',
                 'wind_speed_100m', 'wind_speed_10m']

# Convert wind direction to sine and cosine components in the original DataFrame (df_cleaned)
df_cleaned['wind_direction_sin'] = np.sin(df_cleaned['wind_direction_10m'])
df_cleaned['wind_direction_cos'] = np.cos(df_cleaned['wind_direction_10m'])

# Preprocess the data using scaled_df
def load_and_preprocess_data(scaled_df, df_cleaned, relevant_vars):
    data = scaled_df[relevant_vars].values
    target_sin = df_cleaned['wind_direction_sin'].values
    target_cos = df_cleaned['wind_direction_cos'].values

    # Identify and handle NaNs
    if np.any(np.isnan(data)):
        print("NaNs found in input data. Filling NaNs with column mean.")
        col_means = np.nanmean(data, axis=0)
        inds = np.where(np.isnan(data))
        data[inds] = np.take(col_means, inds[1])

    if np.any(np.isnan(target_sin)):
        print("NaNs found in target_sin. Filling NaNs with mean.")
        target_sin = np.nan_to_num(target_sin, nan=np.nanmean(target_sin))

    if np.any(np.isnan(target_cos)):
        print("NaNs found in target_cos. Filling NaNs with mean.")
        target_cos = np.nan_to_num(target_cos, nan=np.nanmean(target_cos))

    # Convert to sequences
    def to_sequences(data, target_sin, target_cos, seq_size=10):
        x = []
        y_sin = []
        y_cos = []

        for i in range(len(data) - seq_size - 1):
            window = data[i:(i + seq_size)]
            x.append(window)
            y_sin.append(target_sin[i + seq_size])
            y_cos.append(target_cos[i + seq_size])

        return np.array(x), np.array(y_sin), np.array(y_cos)

    seq_size = 10  # Number of time steps to look back
    X, y_sin, y_cos = to_sequences(data, target_sin, target_cos, seq_size)
    return X, y_sin, y_cos

X, y_sin, y_cos = load_and_preprocess_data(scaled_df, df_cleaned, relevant_vars)

# Check for NaNs in the input data after handling
assert not np.any(np.isnan(X)), "Input data contains NaNs"
assert not np.any(np.isnan(y_sin)), "Target sin data contains NaNs"
assert not np.any(np.isnan(y_cos)), "Target cos data contains NaNs"

# Define and train the LSTM model
initial_learning_rate = 0.001

# Learning rate scheduler function
def lr_scheduler(epoch, lr):
    if epoch > 10:
        lr = lr * 0.5  # Reduce the learning rate by half every 10 epochs
    return lr

optimizer = Adam(learning_rate=initial_learning_rate, clipnorm=1.0)
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),
    BatchNormalization(),
    Dropout(0.2),
    LSTM(128),
    BatchNormalization(),
    Dropout(0.2),
    Dense(2)  # Two outputs for sin and cos of wind direction
])
model.compile(optimizer=optimizer, loss='mean_squared_error')

# Implement early stopping and learning rate scheduler
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = LearningRateScheduler(lr_scheduler)

# Train the model with early stopping and learning rate scheduler
history = model.fit(X, np.column_stack((y_sin, y_cos)), epochs=50, batch_size=8192, validation_split=0.2, callbacks=[early_stopping, reduce_lr])

# Evaluate and visualize the model
predictions = model.predict(X)
predictions_sin, predictions_cos = predictions[:, 0], predictions[:, 1]
predictions_direction = np.arctan2(predictions_sin, predictions_cos)
actual_direction = np.arctan2(y_sin, y_cos)

mse = mean_squared_error(actual_direction, predictions_direction)
print(f'Mean Squared Error: {mse}')

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

+import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Ensure GPU is available
if not tf.config.list_physical_devices('GPU'):
    raise SystemError('GPU device not found')
else:
    print('Found GPU at:', tf.test.gpu_device_name())

# Assuming 'scaled_df' and 'df_cleaned' are already prepared as before
# Define relevant variables again if needed
relevant_vars = ['u100', 'v100', 'u10', 'v10', 'fg10', 't2m', 'cbh', 'i10fg', 'mdww', 'msl',
                 'mslhf', 'msshf', 'mwd', 'dwi', 'wind', 'shww', 'sp', 'tcc', 'coriolis',
                 'wind_speed_100m', 'wind_speed_10m']

# Convert wind direction to sine and cosine components in the original DataFrame (df_cleaned)
df_cleaned['wind_direction_sin'] = np.sin(df_cleaned['wind_direction_10m'])
df_cleaned['wind_direction_cos'] = np.cos(df_cleaned['wind_direction_10m'])

# Preprocess the data using scaled_df
def load_and_preprocess_data(scaled_df, df_cleaned, relevant_vars):
    data = scaled_df[relevant_vars].values
    target_sin = df_cleaned['wind_direction_sin'].values
    target_cos = df_cleaned['wind_direction_cos'].values

    # Identify and handle NaNs
    if np.any(np.isnan(data)):
        print("NaNs found in input data. Filling NaNs with column mean.")
        col_means = np.nanmean(data, axis=0)
        inds = np.where(np.isnan(data))
        data[inds] = np.take(col_means, inds[1])

    if np.any(np.isnan(target_sin)):
        print("NaNs found in target_sin. Filling NaNs with mean.")
        target_sin = np.nan_to_num(target_sin, nan=np.nanmean(target_sin))

    if np.any(np.isnan(target_cos)):
        print("NaNs found in target_cos. Filling NaNs with mean.")
        target_cos = np.nan_to_num(target_cos, nan=np.nanmean(target_cos))

    # Convert to sequences
    def to_sequences(data, target_sin, target_cos, seq_size=10):
        x = []
        y_sin = []
        y_cos = []

        for i in range(len(data) - seq_size - 1):
            window = data[i:(i + seq_size)]
            x.append(window)
            y_sin.append(target_sin[i + seq_size])
            y_cos.append(target_cos[i + seq_size])

        return np.array(x), np.array(y_sin), np.array(y_cos)

    seq_size = 10  # Number of time steps to look back
    X, y_sin, y_cos = to_sequences(data, target_sin, target_cos, seq_size)
    return X, y_sin, y_cos

X, y_sin, y_cos = load_and_preprocess_data(scaled_df, df_cleaned, relevant_vars)

# Check for NaNs in the input data after handling
assert not np.any(np.isnan(X)), "Input data contains NaNs"
assert not np.any(np.isnan(y_sin)), "Target sin data contains NaNs"
assert not np.any(np.isnan(y_cos)), "Target cos data contains NaNs"

# Define and train the LSTM model
initial_learning_rate = 0.001

# Learning rate scheduler function
def lr_scheduler(epoch, lr):
    if epoch > 15:
        lr = lr * 0.8  # Reduce the learning rate by 20% every 15 epochs
    return lr

optimizer = Adam(learning_rate=initial_learning_rate, clipnorm=1.0)
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),
    BatchNormalization(),
    Dropout(0.2),
    LSTM(128),
    BatchNormalization(),
    Dropout(0.2),
    Dense(2)  # Two outputs for sin and cos of wind direction
])
model.compile(optimizer=optimizer, loss='mean_squared_error')

# Implement early stopping and learning rate scheduler
early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
reduce_lr = LearningRateScheduler(lr_scheduler)

# Train the model with early stopping and learning rate scheduler
history = model.fit(X, np.column_stack((y_sin, y_cos)), epochs=50, batch_size=8192, validation_split=0.2, callbacks=[early_stopping, reduce_lr])

# Evaluate and visualize the model
predictions = model.predict(X)
predictions_sin, predictions_cos = predictions[:, 0], predictions[:, 1]
predictions_direction = np.arctan2(predictions_sin, predictions_cos)
actual_direction = np.arctan2(y_sin, y_cos)

mse = mean_squared_error(actual_direction, predictions_direction)
print(f'Mean Squared Error: {mse}')

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# code to evaluate on test data

# Assuming X_test, y_sin_test, y_cos_test are your test datasets
# Evaluate the model on test data
predictions_test = model.predict(X_test)
predictions_sin_test, predictions_cos_test = predictions_test[:, 0], predictions_test[:, 1]
predictions_direction_test = np.arctan2(predictions_sin_test, predictions_cos_test)
actual_direction_test = np.arctan2(y_sin_test, y_cos_test)

mse_test = mean_squared_error(actual_direction_test, predictions_direction_test)
print(f'Mean Squared Error on Test Data: {mse_test}')































import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Ensure GPU is available
if not tf.config.list_physical_devices('GPU'):
    raise SystemError('GPU device not found')
else:
    print('Found GPU at:', tf.test.gpu_device_name())

# Assuming 'scaled_df' and 'df_cleaned' are already prepared as before
# Define relevant variables again if needed
relevant_vars = ['u100', 'v100', 'u10', 'v10', 'fg10', 't2m', 'cbh', 'i10fg', 'mdww', 'msl',
                 'mslhf', 'msshf', 'mwd', 'dwi', 'wind', 'shww', 'sp', 'tcc', 'coriolis',
                 'wind_speed_100m', 'wind_speed_10m']

# Convert wind direction to sine and cosine components in the original DataFrame (df_cleaned)
df_cleaned['wind_direction_sin'] = np.sin(df_cleaned['wind_direction_10m'])
df_cleaned['wind_direction_cos'] = np.cos(df_cleaned['wind_direction_10m'])

# Preprocess the data using scaled_df
def load_and_preprocess_data(scaled_df, df_cleaned, relevant_vars):
    data = scaled_df[relevant_vars].values
    target_sin = df_cleaned['wind_direction_sin'].values
    target_cos = df_cleaned['wind_direction_cos'].values

    # Convert to sequences
    def to_sequences(data, target_sin, target_cos, seq_size=10):
        x = []
        y_sin = []
        y_cos = []

        for i in range(len(data) - seq_size - 1):
            window = data[i:(i + seq_size)]
            x.append(window)
            y_sin.append(target_sin[i + seq_size])
            y_cos.append(target_cos[i + seq_size])

        return np.array(x), np.array(y_sin), np.array(y_cos)

    seq_size = 10  # Number of time steps to look back
    X, y_sin, y_cos = to_sequences(data, target_sin, target_cos, seq_size)
    return X, y_sin, y_cos

X, y_sin, y_cos = load_and_preprocess_data(scaled_df, df_cleaned, relevant_vars)

# Check for NaNs in the input data
assert not np.any(np.isnan(X)), "Input data contains NaNs"
assert not np.any(np.isnan(y_sin)), "Target sin data contains NaNs"
assert not np.any(np.isnan(y_cos)), "Target cos data contains NaNs"

# Define and train the LSTM model
initial_learning_rate = 0.001

# Learning rate scheduler function
def lr_scheduler(epoch, lr):
    if epoch > 10:
        lr = lr * 0.5  # Reduce the learning rate by half every 10 epochs
    return lr

optimizer = Adam(learning_rate=initial_learning_rate, clipnorm=1.0)
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),
    BatchNormalization(),
    Dropout(0.2),
    LSTM(128),
    BatchNormalization(),
    Dropout(0.2),
    Dense(2)  # Two outputs for sin and cos of wind direction
])
model.compile(optimizer=optimizer, loss='mean_squared_error')

# Implement early stopping and learning rate scheduler
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = LearningRateScheduler(lr_scheduler)

# Train the model with early stopping and learning rate scheduler
history = model.fit(X, np.column_stack((y_sin, y_cos)), epochs=50, batch_size=8192, validation_split=0.2, callbacks=[early_stopping, reduce_lr])

# Evaluate and visualize the model
predictions = model.predict(X)
predictions_sin, predictions_cos = predictions[:, 0], predictions[:, 1]
predictions_direction = np.arctan2(predictions_sin, predictions_cos)
actual_direction = np.arctan2(y_sin, y_cos)

mse = mean_squared_error(actual_direction, predictions_direction)
print(f'Mean Squared Error: {mse}')

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()









import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Ensure GPU is available
if not tf.config.list_physical_devices('GPU'):
    raise SystemError('GPU device not found')
else:
    print('Found GPU at:', tf.test.gpu_device_name())

# Assuming 'scaled_df' and 'df_cleaned' are already prepared as before
# Define relevant variables again if needed
relevant_vars = ['u100', 'v100', 'u10', 'v10', 'fg10', 't2m', 'cbh', 'i10fg', 'mdww', 'msl',
                 'mslhf', 'msshf', 'mwd', 'dwi', 'wind', 'shww', 'sp', 'tcc', 'coriolis',
                 'wind_speed_100m', 'wind_speed_10m']

# Convert wind direction to sine and cosine components in the original DataFrame (df_cleaned)
df_cleaned['wind_direction_sin'] = np.sin(df_cleaned['wind_direction_10m'])
df_cleaned['wind_direction_cos'] = np.cos(df_cleaned['wind_direction_10m'])

# Preprocess the data using scaled_df
def load_and_preprocess_data(scaled_df, df_cleaned, relevant_vars):
    data = scaled_df[relevant_vars].values
    target_sin = df_cleaned['wind_direction_sin'].values
    target_cos = df_cleaned['wind_direction_cos'].values

    # Convert to sequences
    def to_sequences(data, target_sin, target_cos, seq_size=10):
        x = []
        y_sin = []
        y_cos = []

        for i in range(len(data) - seq_size - 1):
            window = data[i:(i + seq_size)]
            x.append(window)
            y_sin.append(target_sin[i + seq_size])
            y_cos.append(target_cos[i + seq_size])

        return np.array(x), np.array(y_sin), np.array(y_cos)

    seq_size = 10  # Number of time steps to look back
    X, y_sin, y_cos = to_sequences(data, target_sin, target_cos, seq_size)
    return X, y_sin, y_cos

X, y_sin, y_cos = load_and_preprocess_data(scaled_df, df_cleaned, relevant_vars)

# Check for NaNs in the input data
assert not np.any(np.isnan(X)), "Input data contains NaNs"
assert not np.any(np.isnan(y_sin)), "Target sin data contains NaNs"
assert not np.any(np.isnan(y_cos)), "Target cos data contains NaNs"

# Define and train the LSTM model
optimizer = Adam(learning_rate=0.0001, clipnorm=1.0)
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),
    BatchNormalization(),
    Dropout(0.2),
    LSTM(128),
    BatchNormalization(),
    Dropout(0.2),
    Dense(2)  # Two outputs for sin and cos of wind direction
])
model.compile(optimizer=optimizer, loss='mean_squared_error')

# Implement early stopping and learning rate scheduler
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)

# Train the model with early stopping and learning rate scheduler
history = model.fit(X, np.column_stack((y_sin, y_cos)), epochs=50, batch_size=8192, validation_split=0.2, callbacks=[early_stopping, reduce_lr])

# Evaluate and visualize the model
predictions = model.predict(X)
predictions_sin, predictions_cos = predictions[:, 0], predictions[:, 1]
predictions_direction = np.arctan2(predictions_sin, predictions_cos)
actual_direction = np.arctan2(y_sin, y_cos)

mse = mean_squared_error(actual_direction, predictions_direction)
print(f'Mean Squared Error: {mse}')

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()























# Install required packages
!pip install xarray netCDF4 tensorflow

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Ensure GPU is available
if not tf.config.list_physical_devices('GPU'):
    raise SystemError('GPU device not found')
else:
    print('Found GPU at:', tf.test.gpu_device_name())

# Assuming 'scaled_df' and 'df_cleaned' are already prepared as before
# Define relevant variables again if needed
relevant_vars = ['u100', 'v100', 'u10', 'v10', 'fg10', 't2m', 'cbh', 'i10fg', 'mdww', 'msl',
                 'mslhf', 'msshf', 'mwd', 'dwi', 'wind', 'shww', 'sp', 'tcc', 'coriolis',
                 'wind_speed_100m', 'wind_speed_10m']

# Convert wind direction to sine and cosine components in the original DataFrame (df_cleaned)
df_cleaned['wind_direction_sin'] = np.sin(df_cleaned['wind_direction_10m'])
df_cleaned['wind_direction_cos'] = np.cos(df_cleaned['wind_direction_10m'])

# Preprocess the data using scaled_df
def load_and_preprocess_data(scaled_df, df_cleaned, relevant_vars):
    data = scaled_df[relevant_vars].values
    target_sin = df_cleaned['wind_direction_sin'].values
    target_cos = df_cleaned['wind_direction_cos'].values

    # Convert to sequences
    def to_sequences(data, target_sin, target_cos, seq_size=10):
        x = []
        y_sin = []
        y_cos = []

        for i in range(len(data) - seq_size - 1):
            window = data[i:(i + seq_size)]
            x.append(window)
            y_sin.append(target_sin[i + seq_size])
            y_cos.append(target_cos[i + seq_size])

        return np.array(x), np.array(y_sin), np.array(y_cos)

    seq_size = 10  # Number of time steps to look back
    X, y_sin, y_cos = to_sequences(data, target_sin, target_cos, seq_size)
    return X, y_sin, y_cos

X, y_sin, y_cos = load_and_preprocess_data(scaled_df, df_cleaned, relevant_vars)

# Define and train the LSTM model
model = Sequential([
    LSTM(128, activation='relu', return_sequences=True, input_shape=(X.shape[1], X.shape[2])),
    Dropout(0.2),
    LSTM(128, activation='relu'),
    Dropout(0.2),
    Dense(2)  # Two outputs for sin and cos of wind direction
])
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')

# Implement early stopping and learning rate scheduler
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)

# Train the model with early stopping and learning rate scheduler
history = model.fit(X, np.column_stack((y_sin, y_cos)), epochs=50, batch_size=8192, validation_split=0.2, callbacks=[early_stopping, reduce_lr])

# Evaluate and visualize the model
predictions = model.predict(X)
predictions_sin, predictions_cos = predictions[:, 0], predictions[:, 1]
predictions_direction = np.arctan2(predictions_sin, predictions_cos)
actual_direction = np.arctan2(y_sin, y_cos)

mse = mean_squared_error(actual_direction, predictions_direction)
print(f'Mean Squared Error: {mse}')

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()



# Install required packages
!pip install xarray netCDF4 tensorflow

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import xarray as xr
import pandas as pd

# Check if GPU is available
if not tf.config.list_physical_devices('GPU'):
    raise SystemError('GPU device not found')
else:
    print('Found GPU at:', tf.test.gpu_device_name())

# Use the scaled DataFrame 'scaled_df' prepared earlier
# Assuming 'scaled_df' is already available from previous preprocessing steps

# Define relevant variables again if needed
relevant_vars = ['u100', 'v100', 'u10', 'v10', 'fg10', 't2m', 'cbh', 'i10fg', 'mdww', 'msl',
                 'mslhf', 'msshf', 'mwd', 'dwi', 'wind', 'shww', 'sp', 'tcc', 'coriolis',
                 'wind_speed_100m', 'wind_speed_10m']

# Select the target variable (wind direction in radians)
target_var = 'wind_direction_10m'

# Convert wind direction to sine and cosine components in the original DataFrame (df_cleaned)
df_cleaned['wind_direction_sin'] = np.sin(df_cleaned[target_var])
df_cleaned['wind_direction_cos'] = np.cos(df_cleaned[target_var])

# Preprocess the data using scaled_df
def load_and_preprocess_data(scaled_df, df_cleaned, relevant_vars):
    data = scaled_df[relevant_vars].values
    target_sin = df_cleaned['wind_direction_sin'].values
    target_cos = df_cleaned['wind_direction_cos'].values

    # Convert to sequences
    def to_sequences(data, target_sin, target_cos, seq_size=10):
        x = []
        y_sin = []
        y_cos = []

        for i in range(len(data) - seq_size - 1):
            window = data[i:(i + seq_size)]
            x.append(window)
            y_sin.append(target_sin[i + seq_size])
            y_cos.append(target_cos[i + seq_size])

        return np.array(x), np.array(y_sin), np.array(y_cos)

    seq_size = 10  # Number of time steps to look back
    X, y_sin, y_cos = to_sequences(data, target_sin, target_cos, seq_size)
    return X, y_sin, y_cos

X, y_sin, y_cos = load_and_preprocess_data(scaled_df, df_cleaned, relevant_vars)

# Define and train the LSTM model
model = Sequential([
    LSTM(128, activation='relu', return_sequences=True, input_shape=(X.shape[1], X.shape[2])),
    LSTM(128, activation='relu'),
    Dense(2)  # Two outputs for sin and cos of wind direction
])
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X, np.column_stack((y_sin, y_cos)), epochs=50, batch_size=64, validation_split=0.2)

# Evaluate and visualize the model
predictions = model.predict(X)
predictions_sin, predictions_cos = predictions[:, 0], predictions[:, 1]
predictions_direction = np.arctan2(predictions_sin, predictions_cos)
actual_direction = np.arctan2(y_sin, y_cos)

mse = mean_squared_error(actual_direction, predictions_direction)
print(f'Mean Squared Error: {mse}')

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()











# Install required packages
!pip install xarray
!pip install netCDF4

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import xarray as xr

# Check if GPU is available
if not tf.config.list_physical_devices('GPU'):
    raise SystemError('GPU device not found')
else:
    print('Found GPU at:', tf.test.gpu_device_name())

# Load the combined dataset for analysis
combined_file_path = '/content/drive/MyDrive/Sailing Weather Model/Combined_Weather_2020_2023.nc'
combined_ds = xr.open_dataset(combined_file_path)

# Convert xarray dataset to pandas DataFrame
df = combined_ds.to_dataframe().reset_index()

# Calculate Coriolis parameter
omega = 7.2921e-5  # Earth's angular velocity in rad/s
df['coriolis'] = 2 * omega * np.sin(np.deg2rad(df['latitude']))

# Calculate wind speed from U and V components
df['wind_speed_100m'] = np.sqrt(df['u100']**2 + df['v100']**2)
df['wind_speed_10m'] = np.sqrt(df['u10']**2 + df['v10']**2)

# Calculate wind direction from U and V components
df['wind_direction_100m'] = np.arctan2(df['u100'], df['v100'])
df['wind_direction_10m'] = np.arctan2(df['u10'], df['v10'])

# Define relevant variables
relevant_vars = ['u100', 'v100', 'u10', 'v10', 'fg10', 't2m', 'cbh', 'i10fg', 'mdww', 'msl',
                 'mslhf', 'msshf', 'mwd', 'dwi', 'wind', 'shww', 'sp', 'tcc', 'coriolis',
                 'wind_speed_100m', 'wind_speed_10m']

# Ensure that only existing columns are included
relevant_vars = [var for var in relevant_vars if var in df.columns]

# Select the target variable (wind direction in radians)
target_var = 'wind_direction_10m'

# Convert wind direction to sine and cosine components
df['wind_direction_sin'] = np.sin(df[target_var])
df['wind_direction_cos'] = np.cos(df[target_var])

# Preprocess the data
def load_and_preprocess_data(df, relevant_vars):
    data = df[relevant_vars].values
    target_sin = df['wind_direction_sin'].values
    target_cos = df['wind_direction_cos'].values

    # Normalize data
    scaler = MinMaxScaler(feature_range=(0, 1))
    data_scaled = scaler.fit_transform(data)

    # Convert to sequences
    def to_sequences(data, target_sin, target_cos, seq_size=10):
        x = []
        y_sin = []
        y_cos = []

        for i in range(len(data)-seq_size-1):
            window = data[i:(i+seq_size)]
            x.append(window)
            y_sin.append(target_sin[i+seq_size])
            y_cos.append(target_cos[i+seq_size])

        return np.array(x), np.array(y_sin), np.array(y_cos)

    seq_size = 10  # Number of time steps to look back
    X, y_sin, y_cos = to_sequences(data_scaled, target_sin, target_cos, seq_size)
    return X, y_sin, y_cos, scaler

X, y_sin, y_cos, scaler = load_and_preprocess_data(df, relevant_vars)

# Define the LSTM model
def create_model(input_shape):
    model = Sequential([
        LSTM(50, activation='relu', input_shape=input_shape),
        Dropout(0.2),
        Dense(2)  # Two outputs for sin and cos of wind direction
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

# Create the model
model = create_model((X.shape[1], X.shape[2]))

# Summary of the model
model.summary()

# Prepare targets for training
y = np.column_stack((y_sin, y_cos))

# Fit model
history = model.fit(X, y, epochs=20, batch_size=32, validation_split=0.1, verbose=1)

# Make predictions
predictions = model.predict(X)
predictions_sin = predictions[:, 0]
predictions_cos = predictions[:, 1]

# Reconstruct wind direction from sine and cosine components
predictions_direction = np.arctan2(predictions_sin, predictions_cos)
actual_direction = np.arctan2(y_sin, y_cos)

# Evaluate the model
mse = mean_squared_error(actual_direction, predictions_direction)
print(f'Mean Squared Error: {mse}')

# Plot training & validation loss values
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()